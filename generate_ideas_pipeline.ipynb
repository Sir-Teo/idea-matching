{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!python -m spacy download en_core_web_lg"
      ],
      "metadata": {
        "id": "UY0UvNdSJciw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c7c41ebf-e907-42a0-b8f6-d2c8a4daacbb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting en-core-web-lg==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_lg-3.8.0/en_core_web_lg-3.8.0-py3-none-any.whl (400.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m400.7/400.7 MB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: en-core-web-lg\n",
            "Successfully installed en-core-web-lg-3.8.0\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_lg')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from csv import excel\n",
        "import pandas as pd\n",
        "\n",
        "# Load the Excel file\n",
        "excel_path = \"template_Study18_HL_HM.xlsx\"\n",
        "study_name = excel_path.split('.')[0]\n",
        "\n",
        "excel_file = pd.ExcelFile(excel_path)\n",
        "\n",
        "# Get all sheet names\n",
        "sheet_names = excel_file.sheet_names\n",
        "\n",
        "# Export each sheet as a CSV\n",
        "csv_paths = []\n",
        "for sheet in sheet_names:\n",
        "    df = excel_file.parse(sheet)\n",
        "    csv_path = f\"{sheet}.csv\"\n",
        "    df.to_csv(csv_path, index=False)\n",
        "    csv_paths.append(csv_path)\n",
        "\n",
        "csv_paths\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3M8MDnQCJaT6",
        "outputId": "5c790f72-e752-44df-a7a2-825d39ba2b4a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Conversation.csv',\n",
              " 'memory 1.csv',\n",
              " 'memory 2.csv',\n",
              " 'Conversation Idea Units .csv',\n",
              " 'Memory 1 Idea Units .csv',\n",
              " 'Memory 2 Idea Units.csv',\n",
              " 'Memory 1 PT.csv',\n",
              " 'Memory 2 PT.csv',\n",
              " 'Memory Speaker 18-1 PT.csv',\n",
              " 'Memory Speaker 18-2 PT.csv']"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hKM-p58_b474",
        "outputId": "83ffd7c3-b5f4-40d0-de2b-d806ba61f2a4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original text: I think I was 14\n",
            "[Turn 1] I think I was 14\n",
            "---\n"
          ]
        }
      ],
      "source": [
        "import spacy\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_lg\")\n",
        "\n",
        "# Define filler words that we want to remove when they appear as part of a multi-unit turn.\n",
        "FILLERS = {\"oh\", \"ok\", \"um\", \"uh\", \"ah\",\"okay\",'I know','yeah','yep','mhm','Um'}\n",
        "\n",
        "# A set of coordinating conjunctions used for splitting clauses.\n",
        "COORD_CONJS = {\"and\", \"but\", \"so\", \"or\"}\n",
        "\n",
        "# A set of subordinating conjunctions for splitting subordinate clauses.\n",
        "# Here we include both \"because\" and \"when\".\n",
        "SUBORD_CONJS = {\"because\", \"when\"}\n",
        "\n",
        "def is_filler_only(text):\n",
        "    \"\"\"\n",
        "    Returns True if the text (after tokenization) consists only of filler words\n",
        "    (ignoring punctuation and whitespace).\n",
        "    \"\"\"\n",
        "    doc = nlp(text)\n",
        "    tokens = [token.text.lower() for token in doc if not token.is_punct and not token.is_space]\n",
        "    return bool(tokens) and all(token in FILLERS for token in tokens)\n",
        "\n",
        "def extract_idea_units(text, turn_number=1):\n",
        "    \"\"\"\n",
        "    Extracts idea units from a speaker turn while preserving original wording.\n",
        "\n",
        "    For each sentence (as detected by spaCy), we use token offsets to determine\n",
        "    split boundaries based on the following heuristics:\n",
        "      • A new clause is started if a coordinating conjunction (and, but, so, or)\n",
        "        appears (except at the very start).\n",
        "        - For \"but\", we always split.\n",
        "        - For \"and\"/\"so\"/\"or\", we only split if the following token starts a clause\n",
        "          (i.e., has a subject dependency) or is a subordinating marker (dep = \"mark\"),\n",
        "          or if the next token is in SUBORD_CONJS.\n",
        "      • A new clause is also started if a subordinating conjunction such as \"because\"\n",
        "        or \"when\" appears (except at the very start). If \"because\" appears anywhere\n",
        "        in the sentence, ignore \"when\" as a boundary.\n",
        "      • A comma followed by a token with a subject dependency marks a boundary,\n",
        "        except if the comma is preceded by \"like\" (to keep descriptive \"like\" phrases together).\n",
        "\n",
        "    Additionally, any occurrence of the notation \"(inaudible)\" is removed from the idea units.\n",
        "\n",
        "    Finally, if the turn contains multiple idea units, any that are comprised solely\n",
        "    of filler words (e.g., \"oh\", \"ok\", \"um\") are removed. If the turn is only one unit,\n",
        "    it is preserved even if it is just filler.\n",
        "\n",
        "    Additionally, if any idea unit consists solely of connector words such as\n",
        "    \"so\", \"but\", or \"yeah\", it is merged with the following idea unit.\n",
        "\n",
        "    Returns a list of tuples: (turn_number, idea_unit_text)\n",
        "    \"\"\"\n",
        "    text = text.replace(\"\\n\", \" \")\n",
        "    doc = nlp(text)\n",
        "    idea_units = []\n",
        "\n",
        "    # Process sentence by sentence.\n",
        "    for sent in doc.sents:\n",
        "        sent_text = sent.text\n",
        "        sent_start = sent.start_char  # starting char offset of sentence in doc\n",
        "        tokens = list(sent)\n",
        "\n",
        "        # Check if \"because\" is present anywhere in the sentence.\n",
        "        has_because = any(token.text.lower() == \"because\" for token in tokens)\n",
        "\n",
        "        # Collect split boundaries as character indices (relative to the sentence)\n",
        "        boundaries = [0]  # Always start at the beginning\n",
        "\n",
        "        for i, token in enumerate(tokens):\n",
        "            # Calculate the relative index within the sentence.\n",
        "            rel_idx = token.idx - sent_start\n",
        "            token_lower = token.text.lower()\n",
        "\n",
        "            # Heuristic: Subordinate conjunctions (e.g., \"because\" or \"when\")\n",
        "            if token_lower in SUBORD_CONJS and i > 0:\n",
        "                if token_lower == \"because\":\n",
        "                    boundaries.append(rel_idx)\n",
        "                # Only split on \"when\" if \"because\" is not present in the sentence.\n",
        "                elif token_lower == \"when\" and not has_because:\n",
        "                    boundaries.append(rel_idx)\n",
        "\n",
        "            # Heuristic: Coordinating conjunctions\n",
        "            if token_lower in COORD_CONJS and i > 0:\n",
        "                # Always split on \"but\"\n",
        "                if token_lower == \"but\":\n",
        "                    boundaries.append(rel_idx)\n",
        "                else:\n",
        "                    # Split if the following token starts a clause (nsubj/nsubjpass),\n",
        "                    # or if the following token is a subordinating marker (dep_ == \"mark\"),\n",
        "                    # or if the next token is in the set of subordinating conjunctions.\n",
        "                    if i + 1 < len(tokens):\n",
        "                        next_tok = tokens[i+1]\n",
        "                        if (\n",
        "                            next_tok.dep_ in {\"nsubj\", \"nsubjpass\", \"mark\"}\n",
        "                            or next_tok.text.lower() in SUBORD_CONJS\n",
        "                        ):\n",
        "                            boundaries.append(rel_idx)\n",
        "\n",
        "            # Heuristic: Comma boundaries\n",
        "            if token.text == \",\" and i + 1 < len(tokens):\n",
        "                if tokens[i+1].dep_ in {\"nsubj\", \"nsubjpass\"}:\n",
        "                    # Only prevent a split if the token immediately before the comma is \"like\"\n",
        "                    if i > 0 and tokens[i-1].text.lower() != \"like\":\n",
        "                        boundaries.append(rel_idx + len(token.text))\n",
        "\n",
        "\n",
        "\n",
        "        # Ensure the sentence end is included.\n",
        "        if boundaries[-1] != len(sent_text):\n",
        "            boundaries.append(len(sent_text))\n",
        "\n",
        "        # Remove duplicate boundaries and sort them.\n",
        "        boundaries = sorted(set(boundaries))\n",
        "\n",
        "        # Slice the sentence text into segments based on the boundaries.\n",
        "        for j in range(len(boundaries) - 1):\n",
        "             segment = sent_text[boundaries[j]:boundaries[j+1]].strip()\n",
        "             # Remove the notation \"(inaudible)\"\n",
        "             segment = segment.replace(\"(inaudible)\", \"\").strip()\n",
        "             # --- NEW: strip leading/trailing filler tokens ---\n",
        "             words = segment.split()\n",
        "             # drop any filler at the beginning\n",
        "             while words and words[0].lower() in FILLERS:\n",
        "                 words.pop(0)\n",
        "             # drop any filler at the end\n",
        "             while words and words[-1].lower() in FILLERS:\n",
        "                 words.pop()\n",
        "             segment = \" \".join(words).strip()\n",
        "\n",
        "             if segment:\n",
        "                 idea_units.append(segment)\n",
        "\n",
        "    # If there are multiple idea units in the turn, remove those that are filler-only.\n",
        "    if len(idea_units) > 1:\n",
        "        idea_units = [iu for iu in idea_units if not is_filler_only(iu)]\n",
        "\n",
        "    # ----------------- POST PROCESSING: Merge Standalone Connector Segments ----------------- #\n",
        "    CONNECTORS = {\"so\", \"but\", \"yeah\", \"yes\", \"no\", \"anyways\"}\n",
        "    def is_connector_only(segment):\n",
        "        words = [w.strip(\".,!?\").lower() for w in segment.split()]\n",
        "        return bool(words) and all(word in CONNECTORS for word in words)\n",
        "\n",
        "    merged_units = []\n",
        "    i = 0\n",
        "    while i < len(idea_units):\n",
        "        # If the current segment is a connector-only segment,\n",
        "        # merge it with following connector-only segments and the first non-connector segment.\n",
        "        if is_connector_only(idea_units[i]):\n",
        "            group = [idea_units[i]]\n",
        "            i += 1\n",
        "            while i < len(idea_units) and is_connector_only(idea_units[i]):\n",
        "                group.append(idea_units[i])\n",
        "                i += 1\n",
        "            if i < len(idea_units):\n",
        "                group.append(idea_units[i])\n",
        "                i += 1\n",
        "                merged_units.append(\" \".join(group).strip())\n",
        "            else:\n",
        "                # If no non-connector segment follows, merge with the previous unit if available.\n",
        "                if merged_units:\n",
        "                    merged_units[-1] = merged_units[-1] + \" \" + \" \".join(group).strip()\n",
        "                else:\n",
        "                    merged_units.append(\" \".join(group).strip())\n",
        "        else:\n",
        "            merged_units.append(idea_units[i])\n",
        "            i += 1\n",
        "\n",
        "    idea_units = merged_units\n",
        "        # ----------------- NEW POST PROCESSING: Merge Any Very-Short Segment With the Next ----------------- #\n",
        "    merged_incomplete_units = []\n",
        "    i = 0\n",
        "    while i < len(idea_units):\n",
        "        current_unit = idea_units[i]\n",
        "        # If this unit is very short (<3 words) and there's a next unit, merge them.\n",
        "        if i < len(idea_units) - 1 and len(current_unit.split()) < 3:\n",
        "            current_unit = f\"{current_unit} {idea_units[i+1]}\"\n",
        "            merged_incomplete_units.append(current_unit.strip())\n",
        "            i += 2\n",
        "        else:\n",
        "            merged_incomplete_units.append(current_unit)\n",
        "            i += 1\n",
        "\n",
        "    idea_units = merged_incomplete_units\n",
        "\n",
        "\n",
        "    # ----------------- NEW POST PROCESSING: Merge Segments Lacking a Subject ----------------- #\n",
        "    # If an idea unit (other than the first one) does not contain a subject (e.g., \"nsubj\" or \"nsubjpass\"),\n",
        "    # we merge it with the preceding unit.\n",
        "    merged_subject_units = []\n",
        "    for i, unit in enumerate(idea_units):\n",
        "        doc_unit = nlp(unit)\n",
        "        if i > 0 and not any(token.dep_ in {\"nsubj\", \"nsubjpass\"} for token in doc_unit):\n",
        "            merged_subject_units[-1] = merged_subject_units[-1] + \" \" + unit\n",
        "        else:\n",
        "            merged_subject_units.append(unit)\n",
        "    idea_units = merged_subject_units\n",
        "\n",
        "    # Return each idea unit paired with the turn number.\n",
        "    return [(turn_number, iu) for iu in idea_units]\n",
        "\n",
        "\n",
        "# ----------------- SAMPLE USAGE ----------------- #\n",
        "if __name__ == \"__main__\":\n",
        "    texts = [\n",
        "        \"I think I was 14\"\n",
        "    ]\n",
        "\n",
        "    turn_number = 1\n",
        "    for text in texts:\n",
        "        print(f\"Original text: {text}\")\n",
        "        ius = extract_idea_units(text, turn_number)\n",
        "        for turn_num, iu_text in ius:\n",
        "            print(f\"[Turn {turn_num}] {iu_text}\")\n",
        "        print(\"---\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "input_file = \"Conversation.csv\"\n",
        "df = pd.read_csv(input_file)\n",
        "\n",
        "idea_units_data = []\n",
        "cumulative_iu_index = 1  # Start global idea unit counter\n",
        "\n",
        "for _, row in df.iterrows():\n",
        "    turn = row[\"turn\"]\n",
        "    #turn = row['order']\n",
        "    subject = row[\"subject\"]\n",
        "    text = str(row[\"transcript\"]).strip()\n",
        "\n",
        "    if pd.notna(text) and text:  # Only process non-empty transcripts\n",
        "        extracted_units = extract_idea_units(text, turn_number=turn)\n",
        "\n",
        "        # Assign global incremental numbers for idea units across turns\n",
        "        for _, iu_text in extracted_units:\n",
        "            idea_units_data.append([subject, turn, cumulative_iu_index, iu_text])\n",
        "            cumulative_iu_index += 1  # Increment the global idea unit index\n",
        "\n",
        "# Create output DataFrame with cumulative numbering\n",
        "output_df = pd.DataFrame(idea_units_data, columns=[\"Subject Pair\", \"Original Turn\", \"Idea Unit #\", \"Transcript\"])\n",
        "\n",
        "\n",
        "# Save to CSV\n",
        "output_file = study_name + \"_conversation.csv\"\n",
        "output_df.to_csv(output_file, index=False)\n",
        "\n",
        "print(f\"Processed idea units saved to {output_file}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4woqJEGedg9-",
        "outputId": "98264843-8ccc-4575-a9f4-f5a4c0a1dce2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed idea units saved to template_Study18_HL_HM_conversation.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "input_file = \"Memory 1.csv\"\n",
        "df = pd.read_csv(input_file)\n",
        "\n",
        "idea_units_data = []\n",
        "cumulative_iu_index = 1  # Start global idea unit counter\n",
        "\n",
        "for _, row in df.iterrows():\n",
        "    #turn = row[\"turn\"]\n",
        "    turn = row['order']\n",
        "    subject = row[\"subject\"]\n",
        "    text = str(row[\"transcript\"]).strip()\n",
        "\n",
        "    if pd.notna(text) and text:  # Only process non-empty transcripts\n",
        "        extracted_units = extract_idea_units(text, turn_number=turn)\n",
        "\n",
        "        # Assign global incremental numbers for idea units across turns\n",
        "        for _, iu_text in extracted_units:\n",
        "            idea_units_data.append([subject, turn, cumulative_iu_index, iu_text])\n",
        "            cumulative_iu_index += 1  # Increment the global idea unit index\n",
        "\n",
        "# Create output DataFrame with cumulative numbering\n",
        "output_df = pd.DataFrame(idea_units_data, columns=[\"Subject Pair\", \"Original Turn\", \"Idea Unit #\", \"Transcript\"])\n",
        "\n",
        "\n",
        "# Save to CSV\n",
        "output_file = study_name + \"_memory-1.csv\"\n",
        "output_df.to_csv(output_file, index=False)\n",
        "\n",
        "print(f\"Processed idea units saved to {output_file}\")"
      ],
      "metadata": {
        "id": "TOPrD3Q4grXP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e471a2f5-37cc-4ddd-faa9-81a9ba5aa6ed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed idea units saved to template_Study18_HL_HM_memory-1.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "input_file = \"Memory 2.csv\"\n",
        "df = pd.read_csv(input_file)\n",
        "\n",
        "idea_units_data = []\n",
        "cumulative_iu_index = 1  # Start global idea unit counter\n",
        "\n",
        "for _, row in df.iterrows():\n",
        "    #turn = row[\"turn\"]\n",
        "    turn = row['order']\n",
        "    subject = row[\"subject\"]\n",
        "    text = str(row[\"transcript\"]).strip()\n",
        "\n",
        "    if pd.notna(text) and text:  # Only process non-empty transcripts\n",
        "        extracted_units = extract_idea_units(text, turn_number=turn)\n",
        "\n",
        "        # Assign global incremental numbers for idea units across turns\n",
        "        for _, iu_text in extracted_units:\n",
        "            idea_units_data.append([subject, turn, cumulative_iu_index, iu_text])\n",
        "            cumulative_iu_index += 1  # Increment the global idea unit index\n",
        "\n",
        "# Create output DataFrame with cumulative numbering\n",
        "output_df = pd.DataFrame(idea_units_data, columns=[\"Subject Pair\", \"Original Turn\", \"Idea Unit #\", \"Transcript\"])\n",
        "\n",
        "\n",
        "# Save to CSV\n",
        "output_file = study_name + \"_memory-2.csv\"\n",
        "output_df.to_csv(output_file, index=False)\n",
        "\n",
        "print(f\"Processed idea units saved to {output_file}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2TLfNFZuY-cq",
        "outputId": "38d59b1d-a8e8-4dd3-f896-088e139e03e3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed idea units saved to template_Study18_HL_HM_memory-2.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QagCJZUBZqbg"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}